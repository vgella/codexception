import asyncio
import json
import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Optional

from mcp.server.fastmcp import FastMCP
from openai import OpenAI
from pydantic import BaseModel, Field, ValidationError

DEFAULT_CODEX_HANDOFF_TOOLS = [
    "codex",
    "agentic-flow-designer.assess_delegation_need",
    "agentic-flow-designer.design_agentic_solution",
    "agentic-flow-designer.execute_agentic_workflow",
    "agentic-flow-designer.evaluate_agentic_outputs",
    "agentic-flow-designer.summarize_agent_feedback",
    "agentic-flow-designer.revise_agentic_solution",
    "agentic-flow-designer.run_agentic_cycle",
]


def _normalize_tools(available_tools: Optional[List[str]]) -> List[str]:
    """Ensure Codex core + recursive agentic tools are always disclosed to downstream models."""
    merged: List[str] = []
    for tool in DEFAULT_CODEX_HANDOFF_TOOLS + (available_tools or []):
        if tool not in merged:
            merged.append(tool)
    return merged


class SchemaField(BaseModel):
    """Structured description of data exchanged between agents."""

    name: str = Field(..., description="Field identifier, e.g. 'triage_rules'.")
    type: str = Field(..., description="Data type or schema reference.")
    description: str = Field(..., description="Purpose and expected content.")
    required: bool = Field(True, description="Whether the field must be present.")
    source: Optional[str] = Field(
        None,
        description="Upstream agent or system providing this field.",
    )


class AgentAction(BaseModel):
    """Discrete action the agent will perform."""

    step: str = Field(..., description="Short label for the action.")
    description: str = Field(..., description="What the agent does in this step.")
    command: Optional[str] = Field(
        None,
        description="Concrete command/API call to execute (if applicable).",
    )
    produces: List[str] = Field(
        default_factory=list,
        description="Artifacts or schema fields generated by this action.",
    )
    notes: Optional[str] = Field(
        None,
        description="Implementation notes or caveats.",
    )


class AgentEnvironment(BaseModel):
    """Environment prerequisites for an agent."""

    env_vars: List[str] = Field(
        default_factory=list, description="Environment variables required."
    )
    secrets: List[str] = Field(
        default_factory=list, description="Secrets/API keys required."
    )
    validation_commands: List[str] = Field(
        default_factory=list,
        description="Commands to confirm environment readiness.",
    )


class ExecutionNode(BaseModel):
    """Node in the dependency graph representing an agent execution stage."""

    id: str = Field(
        ...,
        description="Unique identifier for this execution node, e.g. 'stage_1'.",
    )
    agent: str = Field(..., description="Name of the agent responsible for this node.")
    description: str = Field(..., description="Purpose of this execution stage.")
    depends_on: List[str] = Field(
        default_factory=list,
        description="IDs of execution nodes that must complete before this one starts.",
    )


class PlannedAgent(BaseModel):
    """Structured view of an agent proposed by the planning phase."""

    name: str = Field(..., description="Concise agent name, e.g. 'spec-writer'.")
    mission: str = Field(..., description="Single sentence describing the agent's purpose.")
    suggested_model: Optional[str] = Field(
        None, description="Optional model recommendation tuned to the agent's responsibilities."
    )
    key_tools: List[str] = Field(
        default_factory=list,
        description="List of tools or APIs the agent should call (empty if default tools suffice).",
    )
    handoff: str = Field(
        ...,
        description="How and when this agent hands off work to the next agent, or 'final' if they conclude the workflow.",
    )
    actions: List[AgentAction] = Field(
        default_factory=list,
        description="Concrete action steps the agent will execute.",
    )
    inputs_schema: List[SchemaField] = Field(
        default_factory=list,
        description="Structured inputs required by the agent.",
    )
    outputs_schema: List[SchemaField] = Field(
        default_factory=list,
        description="Structured outputs produced by the agent.",
    )
    environment: AgentEnvironment = Field(
        default_factory=AgentEnvironment,
        description="Environment expectations for this agent.",
    )


class AgenticFlowPlan(BaseModel):
    """Overall plan returned from the planning agent."""

    summary: str = Field(..., description="High-level summary of the approach.")
    global_context: List[str] = Field(
        default_factory=list,
        description="Shared context / assumptions all agents should know.",
    )
    execution_order: List[str] = Field(
        default_factory=list,
        description="Ordered list describing each major stage the workflow will execute.",
    )
    execution_graph: List[ExecutionNode] = Field(
        default_factory=list,
        description="Dependency graph describing parallelisable execution.",
    )
    shared_artifacts: List[SchemaField] = Field(
        default_factory=list,
        description="Artifacts available across the workflow.",
    )
    agents: List[PlannedAgent] = Field(..., description="Detailed agent definitions.")


class AgentCodeBundle(BaseModel):
    """Code artifact produced by the coding agent."""

    filename: str = Field(..., description="Suggested filename relative to the caller's project.")
    language: str = Field(..., description="Programming language of the generated code.")
    code: str = Field(..., description="Complete source code implementing the workflow.")
    setup_instructions: List[str] = Field(
        default_factory=list, description="Steps the user should take to wire-up the generated code."
    )


class DelegationAssessment(BaseModel):
    """Assessment describing whether a task warrants agentic decomposition."""

    should_delegate: bool
    confidence: float = Field(..., description="Confidence between 0 and 1.")
    reasons: List[str] = Field(default_factory=list)
    suggested_focus: List[str] = Field(default_factory=list)
    estimated_agent_count: Optional[int] = None


class AgentRunOutput(BaseModel):
    """Simulated or observed output from an individual agent execution."""

    agent: str
    status: str
    output_summary: str
    raw_output: Optional[Any] = None
    issues: List[str] = Field(default_factory=list)


class WorkflowExecutionResult(BaseModel):
    """Execution report for the agentic workflow (simulated or actual)."""

    status: str
    notes: str
    per_agent: List[AgentRunOutput]
    artifacts: List[str] = Field(default_factory=list)
    merged_context: Dict[str, Any] = Field(default_factory=dict)


class AgentEvaluation(BaseModel):
    """Evaluation feedback for a single agent."""

    agent: str
    score: float = Field(..., ge=0.0, le=1.0)
    verdict: str
    feedback: str
    blocking_issues: List[str] = Field(default_factory=list)


class WorkflowEvaluation(BaseModel):
    """Aggregate evaluation of the multi-agent run."""

    overall_score: float = Field(..., ge=0.0, le=1.0)
    verdict: str
    issues: List[str] = Field(default_factory=list)
    improvement_feedback: str
    agent_feedback: List[AgentEvaluation] = Field(default_factory=list)


class AgenticCycleIteration(BaseModel):
    """Snapshot of a single iteration in the self-improvement loop."""

    iteration: int
    plan: AgenticFlowPlan
    implementation: AgentCodeBundle
    execution: WorkflowExecutionResult
    evaluation: WorkflowEvaluation
    recommendations: List[str]
    feedback_summary: str


class AgenticCycleResult(BaseModel):
    """Result of running iterative plan-evaluate-revise cycles."""

    converged: bool
    iterations: List[AgenticCycleIteration]
    final_plan: AgenticFlowPlan
    final_implementation: AgentCodeBundle
    final_evaluation: WorkflowEvaluation


class AgenticDesignResult(BaseModel):
    """Combined output from planning and coding phases."""

    plan: AgenticFlowPlan
    implementation: AgentCodeBundle
    additional_recommendations: List[str] = Field(
        default_factory=list, description="Post-processing suggestions or caveats."
    )


def _first_text_content(response: Any) -> str:
    """Extract the first text content from an OpenAI response, supporting both Responses and Chat APIs."""
    # Responses API (preferred)
    output = getattr(response, "output", None)
    if output:
        for item in output:
            for content in getattr(item, "content", []):
                if getattr(content, "type", None) == "output_text":
                    return content.text
                if getattr(content, "type", None) == "text":
                    return getattr(content, "text", "")
    # Chat completions fallback
    choices = getattr(response, "choices", None)
    if choices:
        first = choices[0]
        message = getattr(first, "message", None)
        if message:
            return message.get("content") if isinstance(message, dict) else getattr(message, "content", "")
    raise RuntimeError("Failed to extract text content from OpenAI response.")


def _strip_code_fences(payload: str) -> str:
    """Remove common Markdown code fences to recover raw JSON payloads."""
    text = payload.strip()
    if text.startswith("```") and text.endswith("```"):
        inner = text[3:-3]
        if "\n" in inner:
            first_line, rest = inner.split("\n", 1)
            # Drop language hint lines like "json"
            if first_line.strip() and not first_line.strip().startswith("{"):
                text = rest.strip()
            else:
                text = inner.strip()
        else:
            text = inner.strip()
    return text


def _load_openai_client() -> OpenAI:
    """Create an OpenAI client, allowing optional custom base URL for self-hosted proxies."""
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY is not set. Please export it before using the tool.")
    base_url = os.environ.get("OPENAI_BASE_URL")
    if base_url:
        return OpenAI(api_key=api_key, base_url=base_url)
    return OpenAI(api_key=api_key)


def _truncate_text(text: str, max_chars: int = 6000) -> str:
    """Trim large strings to stay within prompt limits while noting truncation."""
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "\n...<truncated>"


def _invoke_delegation_assessor(
    client: OpenAI,
    task: str,
    model: str,
    temperature: float,
    available_tools: Optional[List[str]] = None,
    complexity_hint: Optional[str] = None,
) -> DelegationAssessment:
    directive = """
You are Codex's task triage specialist. Decide whether delegating sub-tasks to autonomous agents will materially improve quality.
Return compact JSON with keys:
- should_delegate (bool)
- confidence (float 0-1)
- reasons (array of strings)
- suggested_focus (array of strings)
- estimated_agent_count (integer, optional)
Only reference tools explicitly provided.
    """.strip()

    tool_context = ""
    if available_tools:
        tool_context = "Available tools:\n" + "\n".join(f"- {tool}" for tool in available_tools)
    complexity_context = f"Additional context:\n{complexity_hint}" if complexity_hint else ""

    response = client.responses.create(
        model=model,
        temperature=temperature,
        input=[
            {"role": "system", "content": [{"type": "input_text", "text": directive}]},
            {
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": "\n\n".join(
                            part
                            for part in [
                                f"Primary task:\n{task}",
                                tool_context,
                                complexity_context,
                                "Provide the JSON assessment now.",
                            ]
                            if part
                        ),
                    }
                ],
            },
        ],
    )
    payload = _strip_code_fences(_first_text_content(response))
    try:
        data = json.loads(payload)
    except json.JSONDecodeError as exc:
        raise RuntimeError(f"Delegation assessor returned invalid JSON: {exc}\nRaw output:\n{payload}") from exc
    try:
        return DelegationAssessment.model_validate(data)
    except ValidationError as exc:
        raise RuntimeError(f"Delegation assessment failed schema validation: {exc}") from exc


def _invoke_planner(
    client: OpenAI,
    task: str,
    planner_model: str,
    temperature: float,
    available_tools: Optional[List[str]] = None,
    feedback: Optional[str] = None,
    previous_plan: Optional[Dict[str, Any]] = None,
) -> AgenticFlowPlan:
    planner_directive = """
You are Codex's planning specialist. Translate the task into an optimal multi-agent execution blueprint.
Respond strictly as minified JSON with the following top-level keys:
- summary (string)
- global_context (array of strings)
- execution_order (array of strings)
- execution_graph (array of objects with id, agent, description, depends_on array)
- shared_artifacts (array of schema field objects)
- agents (array of objects with the fields:
    name, mission, optional suggested_model, key_tools array, handoff string,
    actions (array of {step, description, optional command, produces array, optional notes}),
    inputs_schema (array of schema field objects),
    outputs_schema (array of schema field objects),
    environment ({env_vars array, secrets array, validation_commands array})
)

Schema field objects must include: name, type, description, required (bool), optional source.
Every agent name must be unique and action-oriented.
Ensure at least one agent covers final validation/handoff.
Respect any provided tool availability; do not invent tools not listed.
Reference dependencies using execution_graph IDs to indicate parallelisable stages.
    """.strip()

    tool_context = ""
    if available_tools:
        tool_context = (
            "Available tools/API integrations:\n"
            + "\n".join(f"- {tool}" for tool in available_tools)
        )

    revision_context = ""
    if previous_plan:
        revision_context = (
            "Prior plan JSON:\n"
            + json.dumps(previous_plan, separators=(",", ":"), ensure_ascii=False)
        )

    feedback_context = ""
    if feedback:
        feedback_context = f"Planner feedback / issues to address:\n{feedback}"

    response = client.responses.create(
        model=planner_model,
        temperature=temperature,
        input=[
            {"role": "system", "content": [{"type": "input_text", "text": planner_directive}]},
            {
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": "\n\n".join(
                            part
                            for part in [
                                f"Primary task:\n{task}",
                                tool_context,
                                revision_context,
                                feedback_context,
                                "Return the JSON plan now.",
                            ]
                            if part
                        ),
                    }
                ],
            },
        ],
    )
    plan_payload = _first_text_content(response)
    plan_payload = _strip_code_fences(plan_payload)
    try:
        plan_dict = json.loads(plan_payload)
    except json.JSONDecodeError as exc:
        raise RuntimeError(f"Planner returned invalid JSON: {exc}\nRaw output:\n{plan_payload}") from exc
    try:
        return AgenticFlowPlan.model_validate(plan_dict)
    except ValidationError as exc:
        raise RuntimeError(f"Planner response failed schema validation: {exc}") from exc


def _simulate_execution(
    client: OpenAI,
    plan: AgenticFlowPlan,
    implementation: AgentCodeBundle,
    task_context: Dict[str, Any],
    available_tools: Optional[List[str]] = None,
) -> WorkflowExecutionResult:
    directive = """
You simulate running a multi-agent workflow using the OpenAI Agents SDK.
Return JSON with keys:
- status (string)
- notes (string)
- per_agent (array of objects with fields agent, status, output_summary, optional raw_output, issues array)
- artifacts (array of strings describing produced artifacts)
- merged_context (object capturing updated shared context)
Reason about likely outcomes based on the plan, code skeleton, and task context.
    """.strip()

    tool_context = ""
    if available_tools:
        tool_context = "Available tools:\n" + "\n".join(f"- {tool}" for tool in available_tools)

    response = client.responses.create(
        model="gpt-4o-mini",
        temperature=0.2,
        input=[
            {"role": "system", "content": [{"type": "input_text", "text": directive}]},
            {
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": "\n\n".join(
                            [
                                "Agentic plan JSON:",
                                json.dumps(plan.model_dump(), indent=2, ensure_ascii=False),
                                "Implementation code (truncated if long):",
                                _truncate_text(implementation.code, 6000),
                                f"Task context: {json.dumps(task_context, indent=2, ensure_ascii=False)}",
                                tool_context,
                                "Simulate execution and return the JSON report.",
                            ]
                        ),
                    }
                ],
            },
        ],
    )
    payload = _strip_code_fences(_first_text_content(response))
    try:
        data = json.loads(payload)
    except json.JSONDecodeError as exc:
        raise RuntimeError(f"Execution simulation returned invalid JSON: {exc}\nRaw output:\n{payload}") from exc
    try:
        return WorkflowExecutionResult.model_validate(data)
    except ValidationError as exc:
        raise RuntimeError(f"Execution simulation failed schema validation: {exc}") from exc


def _invoke_coder(
    client: OpenAI,
    task: str,
    plan: AgenticFlowPlan,
    coder_model: str,
    temperature: float,
    available_tools: Optional[List[str]] = None,
) -> AgentCodeBundle:
    plan_digest_lines = [
        f"- {idx + 1}. {step}" for idx, step in enumerate(plan.execution_order)
    ]
    plan_digest = "\n".join(plan_digest_lines) if plan_digest_lines else "No explicit sequence provided."

    agents_digest = "\n".join(
        f"* {agent.name}: {agent.mission} | Tools: {', '.join(agent.key_tools) or 'default'} | Handoff: {agent.handoff}"
        for agent in plan.agents
    )

    tool_digest = ""
    if available_tools:
        tool_digest = "\n".join(f"- {tool}" for tool in available_tools)

    coder_directive = f"""
You are Codex's senior coding agent. Take the provided multi-agent plan and emit production-ready Python code
using the OpenAI Agents SDK (beta). The code must:
1. Define agent classes or configurations reflecting each planned agent.
2. Register tools where specified (leave TODO comments if external tooling is required).
3. Instantiate a coordinator that wires the planning order: {plan_digest}
4. Implement an execution engine that respects the execution_graph dependency DAG, enabling parallel execution for nodes without dependencies.
5. Expose a `run_workflow(task_context: dict) -> None` entrypoint that handles routing of structured inputs/outputs between agents according to their schema definitions.
6. Validate environment prerequisites (env vars, secrets) before running each agent and surface actionable errors.
7. Include inline guidance on how to extend or execute the workflow.

Return your answer as compact JSON with the keys:
- filename: string (suggested filename under the caller's project root)
- language: string (e.g. "python")
- code: string (the full source file)
- setup_instructions: array of concise strings

Ensure the JSON parses without additional commentary.
    """.strip()

    serialized_plan = json.dumps(plan.model_dump(), indent=2)
    available_tool_text = ""
    if tool_digest:
        available_tool_text = f"\nCaller provided tool inventory:\n{tool_digest}\n"
    response = client.responses.create(
        model=coder_model,
        temperature=temperature,
        input=[
            {"role": "system", "content": [{"type": "input_text", "text": coder_directive}]},
            {
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": f"Primary task:\n{task}\n{available_tool_text}\nStructured plan:\n{serialized_plan}",
                    }
                ],
            },
        ],
    )
    coder_payload = _first_text_content(response)
    coder_payload = _strip_code_fences(coder_payload)
    try:
        coder_dict = json.loads(coder_payload)
    except json.JSONDecodeError as exc:
        raise RuntimeError(f"Coding agent returned invalid JSON: {exc}\nRaw output:\n{coder_payload}") from exc
    try:
        return AgentCodeBundle.model_validate(coder_dict)
    except ValidationError as exc:
        raise RuntimeError(f"Coding agent response failed schema validation: {exc}") from exc


def _invoke_evaluator(
    client: OpenAI,
    plan: AgenticFlowPlan,
    execution: WorkflowExecutionResult,
    evaluation_criteria: Optional[str] = None,
    available_tools: Optional[List[str]] = None,
    model: str = "gpt-4.1-mini",
    temperature: float = 0.2,
) -> WorkflowEvaluation:
    directive = """
You are Codex's evaluation overseer. Score the quality of a multi-agent workflow run.
Return compact JSON with keys:
- overall_score (float 0-1)
- verdict (string; e.g., pass, needs_revision, fail)
- issues (array of strings)
- improvement_feedback (string)
- agent_feedback (array of objects with fields agent, score, verdict, feedback, blocking_issues array)
Reference the execution log and plan when judging outcomes. Highlight blockers clearly.
    """.strip()

    criteria_context = f"Evaluation criteria:\n{evaluation_criteria}" if evaluation_criteria else ""
    tool_context = ""
    if available_tools:
        tool_context = "Relevant tools:\n" + "\n".join(f"- {tool}" for tool in available_tools)

    response = client.responses.create(
        model=model,
        temperature=temperature,
        input=[
            {"role": "system", "content": [{"type": "input_text", "text": directive}]},
            {
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": "\n\n".join(
                            [
                                "Agentic plan JSON:",
                                json.dumps(plan.model_dump(), indent=2, ensure_ascii=False),
                                "Execution report JSON:",
                                json.dumps(execution.model_dump(), indent=2, ensure_ascii=False),
                                criteria_context,
                                tool_context,
                                "Return the evaluation JSON now.",
                            ]
                        ),
                    }
                ],
            },
        ],
    )
    payload = _strip_code_fences(_first_text_content(response))
    try:
        data = json.loads(payload)
    except json.JSONDecodeError as exc:
        raise RuntimeError(f"Evaluation agent returned invalid JSON: {exc}\nRaw output:\n{payload}") from exc
    try:
        return WorkflowEvaluation.model_validate(data)
    except ValidationError as exc:
        raise RuntimeError(f"Evaluation response failed schema validation: {exc}") from exc


def _build_recommendations(
    plan: AgenticFlowPlan, available_tools: Optional[List[str]] = None
) -> List[str]:
    tips: List[str] = []
    if len(plan.agents) > 3:
        tips.append(
            "Consider batching related agents into a single run group if latency is critical."
        )
    if not any("eval" in agent.name.lower() or "qa" in agent.name.lower() for agent in plan.agents):
        tips.append(
            "Add an evaluation/QA agent to harden the workflow before deployment."
        )
    if plan.execution_graph and len(plan.execution_graph) != len(plan.agents):
        tips.append(
            "Align execution_graph nodes with agent coverage to avoid orphaned stages."
        )
    if any(not agent.actions for agent in plan.agents):
        tips.append(
            "Provide explicit action steps and commands for every agent to ensure reproducibility."
        )
    if any(not agent.environment.validation_commands for agent in plan.agents):
        tips.append(
            "Define validation commands per agent to verify environment readiness before execution."
        )
    extras = [
        tool for tool in (available_tools or []) if tool not in DEFAULT_CODEX_HANDOFF_TOOLS
    ]
    if not extras:
        tips.append(
            "Register relevant tools via available_tools input so the plan can reference concrete capabilities."
        )
    tips.append(
        "Review tool access scopes for each agent to ensure least-privilege credentials."
    )
    return tips


def _generate_feedback_summary(evaluation: WorkflowEvaluation) -> str:
    """Summarise evaluation findings for iterative refinement."""
    lines = [
        f"Overall verdict: {evaluation.verdict} (score {evaluation.overall_score:.2f})",
    ]
    if evaluation.issues:
        lines.append("Key issues observed:")
        lines.extend(f"- {issue}" for issue in evaluation.issues)
    blocking = [
        fb for fb in evaluation.agent_feedback if fb.blocking_issues or fb.verdict.lower() in {"fail", "needs_revision"}
    ]
    if blocking:
        lines.append("Blocking agents and required changes:")
        for fb in blocking:
            items = "; ".join(fb.blocking_issues) if fb.blocking_issues else fb.feedback
            lines.append(f"- {fb.agent}: {items}")
    if evaluation.improvement_feedback:
        lines.append("Suggested improvements:")
        lines.append(evaluation.improvement_feedback)
    return "\n".join(lines)


board = FastMCP("agentic-flow-designer")


@board.tool()
async def assess_delegation_need(
    task: str,
    available_tools: Optional[List[str]] = None,
    planner_model: str = "gpt-4o-mini",
    temperature: float = 0.2,
    complexity_hint: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Decide if a task should be decomposed into agentic sub-routines.

    Args:
        task: Natural-language description of the work.
        available_tools: Optional list of additional tools/APIs; Codex core and agentic-flow-designer tools are always available.
        planner_model: Model used for the assessment.
        temperature: Sampling temperature.
        complexity_hint: Extra context about constraints or quality expectations.
    """
    loop = asyncio.get_running_loop()
    task_preview = (task[:120] + "...") if len(task) > 120 else task
    tool_inventory = _normalize_tools(available_tools)
    logging.info(
        "assess_delegation_need called | task preview: %s | tools=%s",
        task_preview.replace("\n", " "),
        ",".join(tool_inventory) if tool_inventory else "none",
    )
    client = await loop.run_in_executor(None, _load_openai_client)
    assessment = await loop.run_in_executor(
        None,
        _invoke_delegation_assessor,
        client,
        task,
        planner_model,
        temperature,
        tool_inventory,
        complexity_hint,
    )
    logging.info(
        "Delegation assessment: should_delegate=%s confidence=%.2f",
        assessment.should_delegate,
        assessment.confidence,
    )
    return assessment.model_dump()


@board.tool()
async def design_agentic_solution(
    task: str,
    planner_model: str = "gpt-4o-mini",
    coder_model: str = "gpt-4.1-mini",
    temperature: float = 0.2,
    available_tools: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """
    Compose an optimal agent workflow for Codex by chaining a planning agent and coding agent.

    Args:
        task: Detailed natural-language description of the user's requirements.
        planner_model: Model identifier for the planning phase.
        coder_model: Model identifier for the coding phase.
        temperature: Generation temperature for both phases.
        available_tools: Additional tools/APIs the workflow may leverage; Codex core and this MCP's endpoints are always included.
    """
    loop = asyncio.get_running_loop()

    task_preview = (task[:120] + "...") if len(task) > 120 else task
    tool_inventory = _normalize_tools(available_tools)
    logging.info(
        "design_agentic_solution called | task preview: %s | planner=%s coder=%s temp=%.2f | tools=%s",
        task_preview.replace("\n", " "),
        planner_model,
        coder_model,
        temperature,
        ",".join(tool_inventory) if tool_inventory else "none",
    )

    client = await loop.run_in_executor(None, _load_openai_client)
    plan = await loop.run_in_executor(
        None,
        _invoke_planner,
        client,
        task,
        planner_model,
        temperature,
        tool_inventory,
        None,
        None,
    )
    logging.info(
        "Planner summary: %s | agents=%d",
        plan.summary,
        len(plan.agents),
    )
    implementation = await loop.run_in_executor(
        None,
        _invoke_coder,
        client,
        task,
        plan,
        coder_model,
        temperature,
        tool_inventory,
    )
    logging.info(
        "Coding agent produced file: %s (%s)",
        implementation.filename,
        implementation.language,
    )

    recommendations = _build_recommendations(plan, tool_inventory)

    result = AgenticDesignResult(
        plan=plan, implementation=implementation, additional_recommendations=recommendations
    )
    logging.info("design_agentic_solution completed successfully.")
    return result.model_dump()


@board.tool()
async def execute_agentic_workflow(
    plan: Dict[str, Any],
    implementation: Dict[str, Any],
    task_context: Optional[Dict[str, Any]] = None,
    available_tools: Optional[List[str]] = None,
    workspace_path: Optional[str] = None,
    simulate_only: bool = True,
) -> Dict[str, Any]:
    """
    Simulate execution of an agentic workflow and optionally persist code locally.

    Args:
        plan: Plan payload from design_agentic_solution/revision.
        implementation: Implementation bundle with filename/code.
        task_context: Runtime context to seed the workflow.
        available_tools: Additional tools/APIs allowed during execution; Codex core and the MCP endpoints are always exposed.
        workspace_path: Optional directory to save the generated code file.
        simulate_only: When True, only simulate execution (default). Real execution is not yet supported.
    """
    loop = asyncio.get_running_loop()
    plan_model = AgenticFlowPlan.model_validate(plan)
    implementation_model = AgentCodeBundle.model_validate(implementation)
    tool_inventory = _normalize_tools(available_tools)
    logging.info(
        "execute_agentic_workflow called | tools=%s | simulate_only=%s | workspace=%s",
        ",".join(tool_inventory) if tool_inventory else "none",
        simulate_only,
        str(workspace_path) if workspace_path else "./agentic_workflows",
    )

    target_dir = Path(workspace_path) if workspace_path else Path.cwd() / "agentic_workflows"
    target_dir.mkdir(parents=True, exist_ok=True)
    file_path = target_dir / implementation_model.filename
    file_path.parent.mkdir(parents=True, exist_ok=True)
    file_path.write_text(implementation_model.code, encoding="utf-8")
    logging.info("Implementation written to %s", file_path)

    if not simulate_only:
        logging.warning("Real execution requested but not supported; falling back to simulation.")

    client = await loop.run_in_executor(None, _load_openai_client)
    execution = await loop.run_in_executor(
        None,
        _simulate_execution,
        client,
        plan_model,
        implementation_model,
        task_context or {},
        tool_inventory,
    )
    logging.info("Simulated execution status: %s", execution.status)
    return {
        "execution": execution.model_dump(),
        "saved_code_path": str(file_path),
        "simulate_only": True,
    }


@board.tool()
async def evaluate_agentic_outputs(
    plan: Dict[str, Any],
    execution_report: Dict[str, Any],
    evaluation_criteria: Optional[str] = None,
    available_tools: Optional[List[str]] = None,
    evaluator_model: str = "gpt-4.1-mini",
    temperature: float = 0.2,
) -> Dict[str, Any]:
    """
    Evaluate agentic workflow outputs and surface actionable feedback.
    """
    loop = asyncio.get_running_loop()
    plan_model = AgenticFlowPlan.model_validate(plan)
    execution_model = WorkflowExecutionResult.model_validate(execution_report)
    tool_inventory = _normalize_tools(available_tools)
    logging.info(
        "evaluate_agentic_outputs called | evaluator=%s | tools=%s",
        evaluator_model,
        ",".join(tool_inventory) if tool_inventory else "none",
    )
    client = await loop.run_in_executor(None, _load_openai_client)
    evaluation = await loop.run_in_executor(
        None,
        _invoke_evaluator,
        client,
        plan_model,
        execution_model,
        evaluation_criteria,
        tool_inventory,
        evaluator_model,
        temperature,
    )
    logging.info(
        "Evaluation verdict: %s (score %.2f)",
        evaluation.verdict,
        evaluation.overall_score,
    )
    return evaluation.model_dump()


@board.tool()
async def summarize_agent_feedback(
    evaluation: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Convert a workflow evaluation payload into concise feedback for replanning.
    """
    evaluation_model = WorkflowEvaluation.model_validate(evaluation)
    summary = _generate_feedback_summary(evaluation_model)
    blocking_agents = [
        fb.agent
        for fb in evaluation_model.agent_feedback
        if fb.blocking_issues or fb.verdict.lower() in {"fail", "needs_revision"}
    ]
    logging.info(
        "summarize_agent_feedback produced summary (%d blocking agents).",
        len(blocking_agents),
    )
    return {
        "summary": summary,
        "blocking_agents": blocking_agents,
        "verdict": evaluation_model.verdict,
        "score": evaluation_model.overall_score,
    }


@board.tool()
async def revise_agentic_solution(
    task: str,
    previous_plan: Dict[str, Any],
    feedback: str,
    planner_model: str = "gpt-4o-mini",
    coder_model: str = "gpt-4.1-mini",
    temperature: float = 0.2,
    available_tools: Optional[List[str]] = None,
    regenerate_code: bool = True,
    previous_implementation: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Request a revised agent workflow based on feedback from a prior run.

    Args:
        task: Primary task description (for context).
        previous_plan: JSON payload returned from an earlier planning call.
        feedback: Description of issues or updates to address.
        planner_model: Model identifier for the planning phase.
        coder_model: Model identifier for the coding phase.
        temperature: Generation temperature for both phases.
        available_tools: Additional tools/APIs available to the workflow; Codex core and the MCP endpoints are always available.
        regenerate_code: If False, reuse previous implementation payload when provided.
        previous_implementation: Existing implementation bundle to reuse when regenerate_code=False.
    """
    loop = asyncio.get_running_loop()
    task_preview = (task[:120] + "...") if len(task) > 120 else task
    tool_inventory = _normalize_tools(available_tools)
    logging.info(
        "revise_agentic_solution called | task preview: %s | feedback len=%d | regenerate_code=%s | tools=%s",
        task_preview.replace("\n", " "),
        len(feedback or ""),
        regenerate_code,
        ",".join(tool_inventory) if tool_inventory else "none",
    )

    client = await loop.run_in_executor(None, _load_openai_client)

    try:
        previous_plan_model = AgenticFlowPlan.model_validate(previous_plan)
    except ValidationError as exc:
        raise RuntimeError(f"previous_plan failed schema validation: {exc}") from exc

    plan = await loop.run_in_executor(
        None,
        _invoke_planner,
        client,
        task,
        planner_model,
        temperature,
        tool_inventory,
        feedback,
        previous_plan_model.model_dump(),
    )
    logging.info(
        "Revised planner summary: %s | agents=%d",
        plan.summary,
        len(plan.agents),
    )

    if regenerate_code or previous_implementation is None:
        implementation = await loop.run_in_executor(
            None,
            _invoke_coder,
            client,
            task,
            plan,
            coder_model,
            temperature,
            tool_inventory,
        )
        logging.info(
            "Revised coding agent produced file: %s (%s)",
            implementation.filename,
            implementation.language,
        )
        implementation_dict = implementation.model_dump()
    else:
        logging.info("Regenerate_code flag disabled; reusing previous implementation bundle.")
        implementation_dict = previous_implementation

    recommendations = _build_recommendations(plan, tool_inventory)
    result = AgenticDesignResult(
        plan=plan,
        implementation=AgentCodeBundle.model_validate(implementation_dict),
        additional_recommendations=recommendations,
    )
    logging.info("revise_agentic_solution completed successfully.")
    return result.model_dump()


@board.tool()
async def run_agentic_cycle(
    task: str,
    available_tools: Optional[List[str]] = None,
    task_context: Optional[Dict[str, Any]] = None,
    max_iterations: int = 3,
    target_score: float = 0.85,
    planner_model: str = "gpt-4o-mini",
    coder_model: str = "gpt-4.1-mini",
    evaluator_model: str = "gpt-4.1-mini",
    temperature: float = 0.2,
    evaluation_criteria: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Run a self-improving loop: plan -> implement -> simulate -> evaluate -> revise.

    Args:
        task: Core objective.
        available_tools: Additional tools the workflow may leverage; Codex core and this MCP's tools are always included.
        task_context: Initial runtime context.
        max_iterations: Upper bound on refinement passes.
        target_score: Score threshold to treat the workflow as successful.
        planner_model/coder_model/evaluator_model: Model overrides for each phase.
        temperature: Shared temperature setting for planning/coding/evaluation.
        evaluation_criteria: Optional rubric or success definition.
    """
    loop = asyncio.get_running_loop()
    client = await loop.run_in_executor(None, _load_openai_client)

    iterations: List[AgenticCycleIteration] = []
    feedback_text: Optional[str] = None
    previous_plan_dump: Optional[Dict[str, Any]] = None

    task_preview = (task[:120] + "...") if len(task) > 120 else task
    tool_inventory = _normalize_tools(available_tools)
    logging.info(
        "run_agentic_cycle started | task preview: %s | tools=%s | max_iterations=%d target_score=%.2f",
        task_preview.replace("\n", " "),
        ",".join(tool_inventory) if tool_inventory else "none",
        max_iterations,
        target_score,
    )

    for iteration in range(1, max_iterations + 1):
        logging.info("Cycle iteration %d planning...", iteration)
        plan = await loop.run_in_executor(
            None,
            _invoke_planner,
            client,
            task,
            planner_model,
            temperature,
            tool_inventory,
            feedback_text,
            previous_plan_dump,
        )
        implementation = await loop.run_in_executor(
            None,
            _invoke_coder,
            client,
            task,
            plan,
            coder_model,
            temperature,
            tool_inventory,
        )
        execution = await loop.run_in_executor(
            None,
            _simulate_execution,
            client,
            plan,
            implementation,
            task_context or {},
            tool_inventory,
        )
        evaluation = await loop.run_in_executor(
            None,
            _invoke_evaluator,
            client,
            plan,
            execution,
            evaluation_criteria,
            tool_inventory,
            evaluator_model,
            temperature,
        )
        feedback_summary = _generate_feedback_summary(evaluation)
        recommendations = _build_recommendations(plan, tool_inventory)

        iteration_record = AgenticCycleIteration(
            iteration=iteration,
            plan=plan,
            implementation=implementation,
            execution=execution,
            evaluation=evaluation,
            recommendations=recommendations,
            feedback_summary=feedback_summary,
        )
        iterations.append(iteration_record)

        logging.info(
            "Iteration %d verdict: %s (score %.2f)",
            iteration,
            evaluation.verdict,
            evaluation.overall_score,
        )

        if evaluation.overall_score >= target_score or evaluation.verdict.lower() == "pass":
            logging.info("Target satisfied at iteration %d.", iteration)
            converged = True
            break

        feedback_text = feedback_summary
        previous_plan_dump = plan.model_dump()
    else:
        converged = False
        logging.info("Reached max iterations without meeting target score %.2f.", target_score)

    final_iteration = iterations[-1]
    cycle_result = AgenticCycleResult(
        converged=converged,
        iterations=iterations,
        final_plan=final_iteration.plan,
        final_implementation=final_iteration.implementation,
        final_evaluation=final_iteration.evaluation,
    )
    return cycle_result.model_dump()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    logging.info("Starting agentic-flow-designer MCP server. Waiting for Codex to connect...")
    board.run()
